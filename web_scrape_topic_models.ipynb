{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping RSS and Topic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime \n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blank dataframe, based on fields identified later\n",
    "\n",
    "rss_feeds = pd.DataFrame(columns = ['title',  'summary',  'links',  'link',  'id',  'guidislink',  'published',  \n",
    "                                    'published_parsed',  'title_detail.type',  'title_detail.language',  \n",
    "                                    'title_detail.base',  'title_detail.value',  'summary_detail.type',  \n",
    "                                    'summary_detail.language',  'summary_detail.base',  'summary_detail.value',  \n",
    "                                    'media_content',  'feedburner_origlink'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of RSS URLs to scrape\n",
    "\n",
    "rss_urls = [r'http://www.schneier.com/blog/index.rdf', \n",
    "            r'http://feeds.feedburner.com/darknethackers', \n",
    "            r'http://securityaffairs.co/wordpress/feed', \n",
    "            r'http://healthitsecurity.com/feed/', \n",
    "            r'http://blog.seanmason.com/feed/', \n",
    "            r'http://threatpost.com/feed', \n",
    "            r'http://feeds.trendmicro.com/Anti-MalwareBlog/', \n",
    "            r'http://www.infosecurity-magazine.com/rss/news/', \n",
    "            r'http://krebsonsecurity.com/feed/', \n",
    "            r'http://www.darkreading.com/rss/all.xml', \n",
    "            r'http://blog.kaspersky.com/feed/', \n",
    "            r'http://www.baesystems.com/page/rss?lg=en', \n",
    "            r'http://rss.nytimes.com/services/xml/rss/nyt/Technology.xml', \n",
    "            r'http://feeds.feedburner.com/scmagazinenews', \n",
    "            r'http://taosecurity.blogspot.com/atom.xml', \n",
    "            r'http://www.rms.com/blog/feed/', \n",
    "            r'http://iscxml.sans.org/rssfeed.xml', \n",
    "            r'https://community.qualys.com/blogs/securitylabs/feeds/posts', \n",
    "            r'http://googleonlinesecurity.blogspot.com/atom.xml', \n",
    "            r'http://thehackernews.com/feeds/posts/default', \n",
    "            r'http://www.us-cert.gov/current/index.rdf', \n",
    "            r'http://feeds.feedburner.com/Securityweek', \n",
    "            r'http://nakedsecurity.sophos.com/feed/', \n",
    "            r'http://feeds.arstechnica.com/arstechnica/index/', \n",
    "            r'http://www.csoonline.com/feed/attribute/41014', \n",
    "            r'http://blogs.rsa.com/feed/', \n",
    "            r'http://feeds.feedburner.com/Techcrunch', \n",
    "            r'http://recode.net/feed/', \n",
    "            r'http://www.techmeme.com/index.xml', \n",
    "            r'http://www.technologyreview.com/stream/rss/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:12<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 items in rss_feed dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the feed entries.  But the dataframe resulting from this has only a summary line, \n",
    "# not the entire text of the article.  For that we will pull the URL in using the \n",
    "# newspaper library later.\n",
    "\n",
    "for rss in tqdm(rss_urls):\n",
    "    feed = feedparser.parse(rss)\n",
    "    rss_feeds=pd.concat([rss_feeds, pd.json_normalize(feed.entries)], axis=0)\n",
    "print(len(rss_feeds), 'items in rss_feed dataframe')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate URLs\n",
    "\n",
    "urllist =rss_feeds.link.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 368/368 [04:58<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 stories in dataframe df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>URL</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2020-04-28 00:00:00</td>\n",
       "      <td>https://www.technologyreview.com/2020/04/28/10...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[disrupting, 5g, potential, industry, manufact...</td>\n",
       "      <td>A continuous stream of emerging technologies i...</td>\n",
       "      <td>A continuous stream of emerging technologies i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>None</td>\n",
       "      <td>https://www.csoonline.com/article/2130877/the-...</td>\n",
       "      <td>[Dan Swinhoe]</td>\n",
       "      <td>[15, data, passwords, users, biggest, century,...</td>\n",
       "      <td>About 3.5 billion people saw their personal da...</td>\n",
       "      <td>Not long ago, a breach that compromised the da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>2020-04-22 00:00:00</td>\n",
       "      <td>https://www.nytimes.com/2020/04/22/technology/...</td>\n",
       "      <td>[Nathaniel Popper]</td>\n",
       "      <td>[times, stimulus, scammer, programs, pure, flo...</td>\n",
       "      <td>“It is a little relief, and then you find out ...</td>\n",
       "      <td>“It is a little relief, and then you find out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>None</td>\n",
       "      <td>https://isc.sans.edu/diary/rss/26054</td>\n",
       "      <td>[Sans Internet Storm Center]</td>\n",
       "      <td>[file, simple, mvpblogdidierstevenscom, handle...</td>\n",
       "      <td>In diary entry \"Obfuscated with a Simple 0x0A\"...</td>\n",
       "      <td>In diary entry \"Obfuscated with a Simple 0x0A\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    date                                                URL  \\\n",
       "360  2020-04-28 00:00:00  https://www.technologyreview.com/2020/04/28/10...   \n",
       "312                 None  https://www.csoonline.com/article/2130877/the-...   \n",
       "135  2020-04-22 00:00:00  https://www.nytimes.com/2020/04/22/technology/...   \n",
       "167                 None               https://isc.sans.edu/diary/rss/26054   \n",
       "\n",
       "                          authors  \\\n",
       "360                            []   \n",
       "312                 [Dan Swinhoe]   \n",
       "135            [Nathaniel Popper]   \n",
       "167  [Sans Internet Storm Center]   \n",
       "\n",
       "                                              keywords  \\\n",
       "360  [disrupting, 5g, potential, industry, manufact...   \n",
       "312  [15, data, passwords, users, biggest, century,...   \n",
       "135  [times, stimulus, scammer, programs, pure, flo...   \n",
       "167  [file, simple, mvpblogdidierstevenscom, handle...   \n",
       "\n",
       "                                               summary  \\\n",
       "360  A continuous stream of emerging technologies i...   \n",
       "312  About 3.5 billion people saw their personal da...   \n",
       "135  “It is a little relief, and then you find out ...   \n",
       "167  In diary entry \"Obfuscated with a Simple 0x0A\"...   \n",
       "\n",
       "                                                  text  \n",
       "360  A continuous stream of emerging technologies i...  \n",
       "312  Not long ago, a breach that compromised the da...  \n",
       "135  “It is a little relief, and then you find out ...  \n",
       "167  In diary entry \"Obfuscated with a Simple 0x0A\"...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get full text using scraping from the newspaper library\n",
    "\n",
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = [\"date\",  \"URL\", \"authors\", \"keywords\", \"summary\", \"text\"])\n",
    "\n",
    "for url in tqdm(urllist):\n",
    "    article = Article(url)\n",
    "    try:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        dict1 = {\"date\": article.publish_date, \"URL\": url, \"authors\": article.authors, \\\n",
    "             \"keywords\": article.keywords, \"summary\": article.summary, \"text\": article.text}\n",
    "    #print(dict1)\n",
    "        df = df.append(dict1, ignore_index=True)\n",
    "    except:\n",
    "        print('Something wrong with', url)\n",
    "\n",
    "print(len(df),'stories in dataframe df')\n",
    "\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368 unique articles in file.\n"
     ]
    }
   ],
   "source": [
    "# Merge the RSS dataframe with the full text obtained from the \n",
    "# newspaper library\n",
    "\n",
    "final = rss_feeds.merge(df,how=\"right\", left_on=\"link\", right_on=\"URL\")\n",
    "print(len(final),'unique articles in file.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle file created\n"
     ]
    }
   ],
   "source": [
    "# Save the file\n",
    "final.to_pickle('securitynews_' + datetime.datetime.now().strftime(\"date_%Y.%m.%d_time_%H.%M\") + '.pkl')\n",
    "print('Pickle file created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"OneZero is tracking thirty countries around the world who are implementing surveillance programs in the wake of COVID-19:\\n\\nThe most common form of surveillance implemented to battle the pandemic is the use of smartphone location data, which can track population-level movement down to enforcing individual quarantines. Some governments are making apps that offer coronavirus health information, while also sharing location information with authorities for a period of time. For instance, in early March, the Iranian government released an app that it pitched as a self-diagnostic tool. While the tool's efficacy was likely low, given reports of asymptomatic carriers of the virus, the app saved location data of millions of Iranians, according to a Vice report.\\n\\nOne of the most alarming measures being implemented is in Argentina, where those who are caught breaking quarantine are being forced to download an app that tracks their location. In Hong Kong, those arriving in the airport are given electronic tracking bracelets that must be synced to their home location through their smartphone's GPS signal.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.text[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and outputs\n",
    "There are just two cells for doing topic modeling.  The first one  \n",
    "specifies the different inputs.  Adjust these to what we need  \n",
    "the topic modeling to do, ie number of topics, top n words per  \n",
    "topic that we wish to see, count vs tfidf, and Non-negative  \n",
    "Matrix Factorization vs Latent Dirichlet Algorithm.  \n",
    "  \n",
    "Once done, the below code will create with two dataframes:  \n",
    "Main output is:  \n",
    " - words_in_topics_df - top_n_words per topic  \n",
    " - topic_for_doc_df - topic to which a document is identified  \n",
    "  \n",
    "Additional outputs of interest:  \n",
    "  \n",
    " - vocab = This is the dict from which you can pull the words, eg vocab['ocean']  \n",
    " - terms = Just the list equivalent of vocab, indexed in the same order  \n",
    " - doc_term_matrix = Document term matrix   \n",
    "Now doc_term_matrix is factorized as = W x H.  You can get W and H:    \n",
    " - W = This matrix has docs as rows and num_topicss as columns  \n",
    " - H = This matrix has num_topics as rows and vocab as columns  \n",
    "  \n",
    "***  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input incoming text as a list called raw_documents\n",
    "\n",
    "raw_documents= list(final['text'])\n",
    "custom_stop_words = joblib.load('stopwords.pkl')\n",
    "# custom_stop_words = stopwords.words('english') - use this from nltk if custom file not available\n",
    "num_topics = 10\n",
    "top_n_words = 10\n",
    "vectorizer_to_use = 3 # Use 1 for CountVectorizer, and 2 for TFIDF_Vectorizer\n",
    "NMF_or_LDA = 'nmf' # Use 'nmf' for NMF or 'lda' for LDA\n",
    "ngram = 3 # 2 for bigrams, 3 for trigrams etc\n",
    "\n",
    "# Once done, the below code will create with two dataframes:\n",
    "# Main output is:\n",
    "#     words_in_topics_df - top_n_words per topic\n",
    "#     topic_for_doc_df - topic to which a document is identified\n",
    "\n",
    "# Additional outputs of interest\n",
    "# vocab = This is the dict from which you can pull the words, eg vocab['ocean']\n",
    "# terms = Just the list equivalent of vocab, indexed in the same order\n",
    "# doc_term_matrix = Document term matrix \n",
    "# doc_term_matrix = W x H\n",
    "# W = This matrix has docs as rows and num_topicss as columns\n",
    "# H = This matrix has num_topics as rows and vocab as columns\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'mon'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 368 X 1589 document-term matrix in variable doc_term_matrix\n",
      "\n",
      "Vocabulary has 1589 distinct terms, examples below \n",
      "['exe boot file', 'execupharm pennsylvania based', 'executable malware compiled', 'execute additional payloads', 'execute anytime user', 'execute arbitrary code', 'execute arbitrary commands', 'execute malicious code', 'executives uber discussing', 'expected close quarter', 'experience trade journalism', 'experts threatpost free', 'explained eset researcher', 'exploit control cybersecurity', 'exploit coronavirus isolation', 'exploit crisis financial', 'exploit flaw sending', 'exploit vulnerabilities control', 'exploit vulnerabilities prevents', 'exploit vulnerability control', 'exploitation vulnerability stealthy', 'exploited remote code', 'exploited sql injection', 'exploits privacy leaks', 'exploits target vulnerabilities', 'exposed redis instances', 'extended security updates', 'external authentication systems', 'external internal threats', 'facebook twitter linkedin', 'factor authentication 2fa', 'failed nonce verification', 'fake delivery notification', 'fake developer profile', 'fake digital passes', 'fake lockdown passes', 'false sense security', 'far_options_page capability requirement', 'fastest growing security', 'features protect ransomware', 'features threat data', 'features time gather', 'federal communications commission', 'federal state local', 'federal trade commission', 'fend latest phishing', 'figure infection chain', 'file data file', 'file inclusion vulnerability', 'file integrity monitoring'] \n",
      "\n",
      "Shape of W is (368, 10) docs as rows and 10 topics as columns. First row below\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Shape of H is (10, 1589) 10 topics as rows and vocab as columns. First row below\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "Top documents for each topic\n",
      "       topic doc_number    weight  \\\n",
      "344  Topic_0        344  0.526640   \n",
      "345  Topic_0        345  0.526640   \n",
      "183  Topic_1        183  0.577259   \n",
      "186  Topic_1        186  0.577259   \n",
      "264  Topic_2        264  0.577441   \n",
      "265  Topic_2        265  0.577441   \n",
      "41   Topic_3         41  0.517394   \n",
      "49   Topic_3         49  0.516243   \n",
      "235  Topic_4        235  0.573287   \n",
      "241  Topic_4        241  0.572544   \n",
      "116  Topic_5        116  0.661016   \n",
      "117  Topic_5        117  0.661016   \n",
      "164  Topic_6        164  0.676208   \n",
      "166  Topic_6        166  0.676208   \n",
      "88   Topic_7         88  0.583735   \n",
      "97   Topic_7         97  0.489772   \n",
      "115  Topic_8        115  0.681332   \n",
      "130  Topic_8        130  0.681332   \n",
      "216  Topic_9        216  0.514855   \n",
      "213  Topic_9        213  0.507164   \n",
      "\n",
      "                                                  text  \n",
      "344  — On the day Layla got out of prison and back ...  \n",
      "345  — On the day Layla got out of prison and back ...  \n",
      "183  The latest news and insights from Google on se...  \n",
      "186  The latest news and insights from Google on se...  \n",
      "264       Have you listened to our podcast? Listen now  \n",
      "265       Have you listened to our podcast? Listen now  \n",
      "41   Researchers say incidents of mobile malware ar...  \n",
      "49   More than 150,000 emails spreading the Hupigon...  \n",
      "235  Juniper has released security updates to addre...  \n",
      "241  Apple has released a security update to addres...  \n",
      "116  When will this end?\\n\\nThis is a difficult que...  \n",
      "117  When will this end?\\n\\nThis is a difficult que...  \n",
      "164  Use our contact form orFor interactive help an...  \n",
      "166  Use our contact form orFor interactive help an...  \n",
      "88   What's Your Cybersecurity Architecture Integra...  \n",
      "97   Cloud Services Are the New Critical Infrastruc...  \n",
      "115  This article is part of the On Tech newsletter...  \n",
      "130  This article is part of the On Tech newsletter...  \n",
      "216  Authentication Bypass\\n\\nCommand Injection\\n\\n...  \n",
      "213  8-Years-Old Apple Zero-Days Exploited in the W...  \n",
      "\n",
      "\n",
      "Topic number and counts of documents against each:\n",
      "Topic_9    189\n",
      "Topic_7     61\n",
      "Topic_3     36\n",
      "Topic_4     29\n",
      "Topic_0     13\n",
      "Topic_6     10\n",
      "Topic_1      9\n",
      "Topic_2      9\n",
      "Topic_5      7\n",
      "Topic_8      5\n",
      "Name: topic, dtype: int64\n",
      "\n",
      "\n",
      "Top 10 words dataframe with weights\n",
      "     topic                     words          freq\n",
      "0  Topic_0  told purchase smartphone  8.491821e-01\n",
      "1  Topic_0       prison georgia told  8.491821e-01\n",
      "2  Topic_0      layla prison georgia  8.491821e-01\n",
      "3  Topic_0     georgia told purchase  8.491821e-01\n",
      "4  Topic_0          day layla prison  8.491821e-01\n",
      "5  Topic_0              mitre att ck  1.295360e-37\n",
      "6  Topic_0          att ck framework  6.834048e-38\n",
      "7  Topic_0       jessica davis april  5.464873e-38\n",
      "8  Topic_0          davis april 2020  5.464873e-38\n",
      "9  Topic_0       contact tracing app  4.016908e-38\n",
      "\n",
      "Same list as above as a list\n",
      "['Topic_0', 'told purchase smartphone', 'prison georgia told', 'layla prison georgia', 'georgia told purchase', 'day layla prison', 'mitre att ck', 'att ck framework', 'jessica davis april', 'davis april 2020', 'contact tracing app']\n",
      "['Topic_1', 'google security safety', 'news insights google', 'security safety internet', 'insights google security', 'latest news insights', 'davis april 2020', 'jessica davis april', 'contact tracing app', 'department homeland security', 'american hospital association']\n",
      "['Topic_2', 'listened podcast listen', 'https docs google', 'forms 1faipqlse8akymfaawj4jzzyrm8gfsjcdon8q83c9_wu5u10snat_cca viewform', 'vote winners https', 'vote european cybersecurity', 'viewform pierluigi paganini', 'winners https docs', 'cybersecurity blogger awards', 'docs google forms', 'awards vote winners']\n",
      "['Topic_3', 'business email compromise', 'fend latest phishing', 'security experts threatpost', 'inbox fend latest', 'inbox security defense', 'free webinar proven', 'advanced takeaways lockdown', 'today fastest growing', 'growing security threat', 'prevent email compromise']\n",
      "['Topic_4', 'agency cisa encourages', 'security agency cisa', 'cybersecurity infrastructure security', 'infrastructure security agency', 'cisa encourages users', 'users administrators review', 'encourages users administrators', 'control cybersecurity infrastructure', 'attacker exploit vulnerability', 'remote attacker exploit']\n",
      "['Topic_5', 'hospitalization resorting crisis', 'conduct monitoring confirmed', 'confirmed cases contacts', 'virus contained question', 'watson staked goal', 'contained question reopen', 'country american enterprise', 'resorting crisis standards', 'requiring hospitalization resorting', 'lot depends virus']\n",
      "['Topic_6', 'users slack group', 'chat users slack', 'contact form orfor', 'interactive chat users', 'orfor interactive chat', 'form orfor interactive', 'forms 1faipqlse8akymfaawj4jzzyrm8gfsjcdon8q83c9_wu5u10snat_cca viewform', 'blogger awards vote', 'pierluigi paganini securityaffairs', 'vote winners https']\n",
      "['Topic_7', 'dark reading omdia', 'reading omdia analysts', 'analysts meet challenges', 'omdia analysts meet', 'meet challenges covid', 'products services compiled', 'compiled dark reading', 'services compiled dark', 'free products services', 'listing free products']\n",
      "['Topic_8', 'newsletter sign receive', 'article tech newsletter', 'tech newsletter sign', 'sign receive weekdays', 'receive weekdays facebook', 'winners https docs', 'forms 1faipqlse8akymfaawj4jzzyrm8gfsjcdon8q83c9_wu5u10snat_cca viewform', 'linkedin reddit pinterest', 'pierluigi paganini securityaffairs', 'vote winners https']\n",
      "['Topic_9', 'remote code execution', 'comment share facebook', 'share facebook twitter', 'twitter linkedin group', 'article comment share', 'facebook twitter linkedin', 'arbitrary code execution', 'arbitrary file download', 'code execution vulnerability', 'gain unauthorized access']\n"
     ]
    }
   ],
   "source": [
    "# use count based vectorizer\n",
    "if vectorizer_to_use ==1:\n",
    "    vectorizer = CountVectorizer(stop_words = custom_stop_words, min_df = 2, analyzer='word', ngram_range=(ngram, ngram))\n",
    "else:\n",
    "    # or use TF-IDF based vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=custom_stop_words, analyzer='word', ngram_range=(ngram, ngram))\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix in variable doc_term_matrix\\n\" % (doc_term_matrix.shape[0], doc_term_matrix.shape[1]) )\n",
    "\n",
    "vocab = vectorizer.vocabulary_ #This is the dict from which you can pull the words, eg vocab['ocean']\n",
    "terms = vectorizer.get_feature_names() #Just the list equivalent of vocab, indexed in the same order\n",
    "print(\"Vocabulary has %d distinct terms, examples below \" % len(terms))\n",
    "print(terms[500:550], '\\n')\n",
    "\n",
    "\n",
    "# create the model\n",
    "# Pick between NMF or LDA methods (don't know what they are, try whichever gives better results)\n",
    "if NMF_or_LDA == 'nmf':\n",
    "    model = NMF( init=\"nndsvd\", n_components=num_topics ) \n",
    "else:\n",
    "    model = LatentDirichletAllocation(n_components=num_topics, learning_method='online') \n",
    "    \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( doc_term_matrix ) #This matrix has docs as rows and k-topics as columns\n",
    "H = model.components_ #This matrix has k-topics as rows and vocab as columns\n",
    "print('Shape of W is', W.shape, 'docs as rows and', num_topics, 'topics as columns. First row below')\n",
    "print(W[0].round(1))\n",
    "print('\\nShape of H is', H.shape, num_topics, 'topics as rows and vocab as columns. First row below')\n",
    "print(H[0].round(1))\n",
    "\n",
    "# Check which document belongs to which topic, and print value_count\n",
    "topic_for_doc_df = pd.DataFrame(columns = ['article', 'topic', 'value'])\n",
    "for i in range(W.shape[0]):\n",
    "    a = W[i] \n",
    "    b = np.argsort(a)[::-1]\n",
    "    temp_df = pd.DataFrame({'article': [i], 'topic':['Topic_'+str(b[0])], 'value': [a[b[0]]]})\n",
    "    topic_for_doc_df = pd.concat([topic_for_doc_df, temp_df])\n",
    "\n",
    "top_docs_for_topic_df = pd.DataFrame(columns = ['topic', 'doc_number', 'weight'])    \n",
    "for i in range(W.shape[1]):\n",
    "    topic = i\n",
    "    temp_df = pd.DataFrame({'topic': ['Topic_'+str(i) for x in range(W.shape[0])], \n",
    "                            'doc_number':  list(range(W.shape[0])), \n",
    "                            'weight': list(W[:,i])})\n",
    "    temp_df = temp_df.sort_values(by=['topic', 'weight'], ascending=[True, False])\n",
    "    top_docs_for_topic_df = pd.concat([top_docs_for_topic_df, temp_df])\n",
    "# Add text to the top_docs dataframe as a new column\n",
    "top_docs_for_topic_df['text']=[raw_documents[i] for i in list(top_docs_for_topic_df.doc_number)] \n",
    "# Print top two docs for each topic\n",
    "print('\\nTop documents for each topic')\n",
    "print(top_docs_for_topic_df.groupby('topic').head(2))\n",
    "\n",
    "print('\\n')\n",
    "print('Topic number and counts of documents against each:')\n",
    "print(topic_for_doc_df.topic.value_counts())\n",
    "\n",
    "# Create dataframe with top-10 words for each topic\n",
    "words_in_topics_df = pd.DataFrame(columns = ['topic', 'words', 'freq'])\n",
    "for i in range(H.shape[0]):\n",
    "    a = H[i] \n",
    "    b = np.argsort(a)[::-1]\n",
    "    np.array(b[:top_n_words])\n",
    "    words = [terms[i] for i in b[:top_n_words]]\n",
    "    freq = [a[i] for i in b[:top_n_words]]\n",
    "    temp_df = pd.DataFrame({'topic':'Topic_'+str(i), 'words': words, 'freq': freq})\n",
    "    words_in_topics_df = pd.concat([words_in_topics_df, temp_df])\n",
    "\n",
    "print('\\n')\n",
    "print('Top', top_n_words, 'words dataframe with weights')\n",
    "print(words_in_topics_df.head(10))\n",
    "\n",
    "\n",
    "\n",
    "# print as list\n",
    "print('\\nSame list as above as a list')\n",
    "words_in_topics_list = words_in_topics_df.groupby('topic')['words'].apply(list)\n",
    "lala =[]\n",
    "for i in range(len(words_in_topics_list)):\n",
    "    a = [list(words_in_topics_list.index)[i]]\n",
    "    b = words_in_topics_list[i]\n",
    "    lala = lala + [a+b]\n",
    "    print(a + b) \n",
    "    \n",
    "    \n",
    "# Top docs for a topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
