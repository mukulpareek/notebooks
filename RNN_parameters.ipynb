{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting parameters for an RNN  \n",
    "  \n",
    "In order to get the RNN to consume text data, we need two things:  \n",
    "  \n",
    "1. A list of all the texts, eg `texts = ['Today is a nice day', 'Yesterday was gorgeous',....]`  \n",
    "2. We need the labels in a list too, eg `labels = [1, 0, 1, 1,...]`  \n",
    "  \n",
    "We tokenize and do other preprocessing to get tensors for the data and the labels to feed into our NN.  \n",
    "  \n",
    "We use pretrained Glove embeddings to build the Embeddings layer.  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the `embedding_matrix` has one row per word in the vocabulary.  Each row has the vector for that word, picked from glove.  Because it is an np.array, it has no row or column names. The order of the words in the rows is the same as the order of words in the dict word_index.  \n",
    "  \n",
    "We will feed this embedding matrix as weights to the embedding layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, SimpleRNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Embedding Layer works  \n",
    "The embedding layer has three parameters.  \n",
    "   \n",
    "**Arguments to specify**\n",
    "1. input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10000, then the size of the vocabulary would be 10001 words (because the first element is not used and is all zeros).\n",
    "2. output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. In the current example it is 100 long.\n",
    "3. input_length - optional parameter, default is 'None': This is the length of input sequences, as you would define for any input layer of a Keras model. In the current case we are using maxlen=100.\n",
    "  \n",
    "**Param count**  \n",
    "The total count of trainable parameters will be vocab_size * embedding_depth.  \n",
    "  \n",
    "The Embeddings Layer is just a weights matrix - with dimensions equal to the (vocab_size, embedding_depth).  In our case we have vocab_size=100, and embedding_dim=100 so we will have 100\\*100 weights which means the count of parameters is 10,000.  \n",
    "  \n",
    "**Output shape**  \n",
    "The output shape will be maxlen * embedding_depth.  \n",
    "  \n",
    "The Embedding Layer will take every single observation, which has a length of 100 (maxlen), and replace each numerical number with the corresponding weight from the embedding matrix.  Which means the output of the embedding layer will be (maxlen, embedding_depth), or in this case 100\\*100, also 10,000.  That is its output shape, which feeds into the next layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters for illustration\n",
    "vocab_size, embedding_dim, maxlen= 10000, 100, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen)) # Note that vocab_size=10000, embedding_dim = 100 (100 dense vector for each word from Glove), maxlen=100 (using only first 100 words of each review)\n",
    "model.add(Flatten()) # Get flat layer equal to the output size of the prior layer\n",
    "model.add(Dense(32, activation = 'relu')) # Dense layer with 32 nodes. So param # = (Prior layer output size * 32) + 32 biases\n",
    "model.add(Dense(1, activation='sigmoid')) # Dense layer with 1 node. So param # = (Prior layer output size * 1) + 1 bias\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN\n",
    "  \n",
    "The parameter count for a SimpleRNN layer is given by:  \n",
    "\n",
    "  **((InputSize + Number of nodes) * Number of nodes)  +  Number of nodes**  \n",
    "\n",
    "In a simple RNN layer (that has no memory cell, or $\\tilde{c}$, like GRU or LSTM, two things are input: the recurrent activation $a^{<t-1>}$ from the prior cell, and the $x^{<t>}$.  \n",
    "  \n",
    "Then there are two things output: the activation of the SimpleRNN cell, or $a^{<t-1>}$, and $\\hat{y}^{<t>}$.  \n",
    "  \n",
    "$a^{<t>} = g(W_a[a^{<t-1>}, x^{<t>}] + b_a)$\n",
    "and  \n",
    "$\\hat{y}^{<t>} = g'(W_{ya}a^{<t>} + b_y)$ \n",
    "  \n",
    "(where $g$ and $g'$ are activation functions that can be different from one another.)  \n",
    "  \n",
    "Now $[a^{<t-1>}, x^{<t>}]$ is the side-by-side stacking of $a^{<t-1>}$ and $x^{<t>}$.  \n",
    "  \n",
    "All the $a^{<n>}$s are the same in size, and are really just the count of the output nodes of the RNN, in this case 32.  The $x^{<n>}$s are all words, equal in size to the embedding length, in this case 100.  So the side-by-side stacking of $a^{<t-1>}$ and $x^{<t>}$ is 132 in length.  Therefore $W_a$ is also 132 in length.  \n",
    "\n",
    "So now you have an incoming vector of dimension (132,), being multiplied by 32 nodes inside the SimpleRNN cell, giving us 132 * 32 = 4224 parameters.  Add 32 biases to that, and 4224 + 32 = 4256.  Which gives us the 4256 parameter count as above.\n",
    "  \n",
    "In the second equation, $W_{ya}$ has the same dimensions as $a^{<t>}$, which is 32.  So this has 32 * 32 = 1024 parameters, plus 32 biases which should give us 1056 additional parameters which seem to be not counted in the Keras calculation above.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                4256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,004,289\n",
      "Trainable params: 1,004,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) # Note that vocab_size=10000 (vocab size), embedding_dim = 100 (100 dense vector for each word from Glove), maxlen=100 (using only first 100 words of each review)\n",
    "model.add(SimpleRNN(32))\n",
    "#model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM parameters\n",
    "In an LSTM layer, the parameter count is similarly done, except that there are 4 equations so everything we did for the SimpleRNN needs to be multiplied by 4.  \n",
    "\n",
    "**[((InputSize + Number of nodes) * Number of nodes) + Number of nodes] * 4**\n",
    "  \n",
    "An LSTM has the following equations for its various things.\n",
    "\n",
    "Inputs:  \n",
    "1. Prior activation ($a^{<t-1>}$), which is identical to the node size of the LSTM  \n",
    "2. Input $x^{<t>}$, ie the word, represented as a vector  \n",
    "3. Prior $\\tilde{c}^{<t-1>}$, which is the candidate memory cell\n",
    "\n",
    "Outputs:\n",
    "1. Memory cell $\\tilde{c}^{<t>}$\n",
    "2. Activation $a^{<t>}$\n",
    "  \n",
    "$\\tilde{c}^{<t>} = tanh(W_c[a^{<t-1>}, x^{<t>}) + b_c)$ : **Candidate memory cell**    \n",
    "$\\Gamma_u = \\sigma(W_u[a^{<t-1>}, x^{<t>}) + b_u)$ : **UPDATE GATE**  \n",
    "$\\Gamma_f = \\sigma(W_f[a^{<t-1>}, x^{<t>}) + b_u)$ : **FORGET GATE**  \n",
    "$\\Gamma_o = \\sigma(W_o[a^{<t-1>}, x^{<t>}) + b_u)$ : **OUTPUT GATE**  \n",
    "$c^{<t>} = \\Gamma_u * \\tilde{c}^{<t>} + \\Gamma_f * \\tilde{c}^{<t-1>}$  : **MEMORY CELL OUTPUT** \n",
    "$a^{<t>} = \\Gamma_o * tanh(c^{<t>})$  : **ACTIVATION OUTPUT**  \n",
    "\n",
    "$\\sigma$ is the sigmoid function  \n",
    "\n",
    "Now each one of W_c, W_u, W_f, and W_o will have the dimensions determined by the concatenation of a^<t-1> and x^t.  That will be Node Size + Input size, multiplied by the number of nodes (as it behaves like a fully connected layer).  Add to that the number of biases (equal to the number of nodes), and because there are 4 of these, multiply in the end by 4.  \n",
    "    \n",
    "For the example above, input size is 100 (embedding size), and number of nodes is 32, therefore the count of parameters is =(((100+32) * 32)+32) * 4 = 17024.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,017,057\n",
      "Trainable params: 1,017,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) # Note that vocab_size=10000 (vocab size), embedding_dim = 100 (100 dense vector for each word from Glove), maxlen=100 (using only first 100 words of each review)\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the model**\n",
    "```python\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
