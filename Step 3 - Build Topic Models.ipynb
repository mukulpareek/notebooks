{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import feedparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import datetime \n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import joblib\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and outputs\n",
    "There are just two cells for doing topic modeling.  The first one  \n",
    "specifies the different inputs.  Adjust these to what we need  \n",
    "the topic modeling to do, ie number of topics, top n words per  \n",
    "topic that we wish to see, count vs tfidf, and Non-negative  \n",
    "Matrix Factorization vs Latent Dirichlet Algorithm.  \n",
    "  \n",
    "Once done, the below code will create with two dataframes:  \n",
    "Main output is:  \n",
    " - words_in_topics_df - top_n_words per topic  \n",
    " - topic_for_doc_df - topic to which a document is identified  \n",
    "  \n",
    "Additional outputs of interest:  \n",
    "  \n",
    " - vocab = This is the dict from which you can pull the words, eg vocab['ocean']  \n",
    " - terms = Just the list equivalent of vocab, indexed in the same order  \n",
    " - doc_term_matrix = Document term matrix   \n",
    "Now doc_term_matrix is factorized as = W x H.  You can get W and H:    \n",
    " - W = This matrix has docs as rows and num_topicss as columns  \n",
    " - H = This matrix has num_topics as rows and vocab as columns  \n",
    "  \n",
    "***  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = joblib.load('final_df3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stopwords from file\n",
    "custom_stop_words = []\n",
    "file = open(file = \"stopwords.txt\", mode = 'r')\n",
    "custom_stop_words = file.read().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How to write to a file\n",
    "# # Uncomment everything in this cell to write to a file\n",
    "# file = open('stopwords.txt','w')\n",
    "# for element in custom_stop_words:\n",
    "#     file.write(element+'\\n')\n",
    "\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input incoming text as a list called raw_documents\n",
    "\n",
    "raw_documents= list(final['text'])\n",
    "# custom_stop_words = joblib.load('stopwords.pkl')\n",
    "# custom_stop_words = stopwords.words('english') - use this from nltk if custom file not available\n",
    "num_topics = 10\n",
    "top_n_words = 20\n",
    "vectorizer_to_use = 1 # Use 1 for CountVectorizer, and 2 for TFIDF_Vectorizer\n",
    "NMF_or_LDA = 'nmf' # Use 'nmf' for NMF or 'lda' for LDA\n",
    "ngram = 2 # 2 for bigrams, 3 for trigrams etc\n",
    "\n",
    "# Once done, the below code will create with two dataframes:\n",
    "# Main output is:\n",
    "#     words_in_topics_df - top_n_words per topic\n",
    "#     topic_for_doc_df - topic to which a document is identified\n",
    "\n",
    "# Additional outputs of interest\n",
    "# vocab = This is the dict from which you can pull the words, eg vocab['ocean']\n",
    "# terms = Just the list equivalent of vocab, indexed in the same order\n",
    "# term_frequency_table = dataframe with the frequency of terms\n",
    "# doc_term_matrix = Document term matrix \n",
    "# doc_term_matrix = W x H\n",
    "# W = This matrix has docs as rows and num_topicss as columns\n",
    "# H = This matrix has num_topics as rows and vocab as columns\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5125 X 199469 document-term matrix in variable doc_term_matrix\n",
      "\n",
      "Vocabulary has 199469 distinct terms, examples below \n",
      "['100 days', '100 dollars', '100 donation', '100 downloads', '100 eagle', '100 efficient', '100 employees', '100 engineering', '100 extraterrestrial', '100 false', '100 final', '100 follow', '100 fortune', '100 frontline', '100 fund', '100 funding', '100 games', '100 gigabits', '100 high', '100 hosts', '100 increase', '100 internet', '100 investment', '100 jobs', '100 kwh', '100 meetings', '100 mid', '100 mile', '100 miles', '100 millisecond', '100 minutes', '100 mw', '100 networks', '100 organizations', '100 participants', '100 party', '100 people', '100 percent', '100 popular', '100 private', '100 proof', '100 quarter', '100 quest', '100 reliable', '100 remote', '100 round', '100 russian', '100 security', '100 series', '100 spam'] \n",
      "\n",
      "Shape of W is (5125, 10) docs as rows and 10 topics as columns. First row below\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Shape of H is (10, 199469) 10 topics as rows and vocab as columns. First row below\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "\n",
      "Top documents for each topic\n",
      "        topic doc_number     weight  \\\n",
      "1353  Topic_0       1353   9.715913   \n",
      "3725  Topic_0       3725   7.916274   \n",
      "3076  Topic_1       3076  15.573830   \n",
      "2610  Topic_1       2610   0.512338   \n",
      "2961  Topic_2       2961   6.871305   \n",
      "4235  Topic_2       4235   6.847570   \n",
      "97    Topic_3         97   3.684187   \n",
      "540   Topic_3        540   3.666510   \n",
      "441   Topic_4        441   9.075249   \n",
      "349   Topic_4        349   7.780611   \n",
      "3769  Topic_5       3769  15.097542   \n",
      "58    Topic_5         58   0.333696   \n",
      "4581  Topic_6       4581   6.301879   \n",
      "2497  Topic_6       2497   6.053257   \n",
      "1325  Topic_7       1325   2.533108   \n",
      "4214  Topic_7       4214   2.448669   \n",
      "3852  Topic_8       3852   9.409006   \n",
      "4991  Topic_8       4991   0.488396   \n",
      "2216  Topic_9       2216   5.039252   \n",
      "2334  Topic_9       2334   5.036709   \n",
      "\n",
      "                                                   text  \n",
      "1353  This month we got patches for 130 vulnerabilit...  \n",
      "3725  This month we got an average Patch Tuesday wit...  \n",
      "3076  ZLab researchers spotted a new malicious espio...  \n",
      "2610  Attackers are always trying to find new ways t...  \n",
      "2961  About: Our mission is to be the reliable, skil...  \n",
      "4235  About: Our mission is to be the reliable, skil...  \n",
      "97    COVID-19: Latest Security News & Commentary\\n\\...  \n",
      "540   COVID-19: Latest Security News & Commentary\\n\\...  \n",
      "441   kevin roose\\n\\nIn the summer of 2014, Michael ...  \n",
      "349   kevin roose\\n\\nIn the summer of 2014, Michael ...  \n",
      "3769  Microsoft May 2020 Patch Tuesday security upda...  \n",
      "58    Updated on May 20, 2020, 1:45 P.M. PST to incl...  \n",
      "4581  There is nothing like attending a face-to-face...  \n",
      "2497  There is nothing like attending a face-to-face...  \n",
      "1325  As a world afflicted by the coronavirus pandem...  \n",
      "4214  Contact tracing apps for COVID-19 (coronavirus...  \n",
      "3852  Yesterday, Bojan wrote a nice diary[1] about t...  \n",
      "4991  A joint report released by the U.S. NSA and th...  \n",
      "2216  We spent this weekend going hands-on with a pa...  \n",
      "2334  We spent this weekend going hands-on with a pa...  \n",
      "\n",
      "\n",
      "Topic number and counts of documents against each:\n",
      "Topic_7    2625\n",
      "Topic_3     619\n",
      "Topic_6     536\n",
      "Topic_4     387\n",
      "Topic_2     268\n",
      "Topic_9     256\n",
      "Topic_0     240\n",
      "Topic_8     100\n",
      "Topic_5      52\n",
      "Topic_1      42\n",
      "Name: topic, dtype: int64\n",
      "\n",
      "\n",
      "Top 20 words dataframe with weights\n",
      "     topic                     words       freq\n",
      "0  Topic_0                  cve 2020  14.491005\n",
      "1  Topic_0         vulnerability cve   6.650055\n",
      "2  Topic_0       elevation privilege   2.645550\n",
      "3  Topic_0   privilege vulnerability   2.597222\n",
      "4  Topic_0            code execution   2.101164\n",
      "5  Topic_0               remote code   1.749153\n",
      "6  Topic_0   execution vulnerability   1.342613\n",
      "7  Topic_0  disclosure vulnerability   0.931998\n",
      "8  Topic_0         service elevation   0.708544\n",
      "9  Topic_0            denial service   0.683493\n",
      "\n",
      "Same list as above as a list\n",
      "['Topic_0', 'cve 2020', 'vulnerability cve', 'elevation privilege', 'privilege vulnerability', 'code execution', 'remote code', 'execution vulnerability', 'disclosure vulnerability', 'service elevation', 'denial service', 'service vulnerability', 'memory corruption', 'microsoft sharepoint', 'critical cve', 'specially crafted', 'critical microsoft', 'corruption vulnerability', 'spoofing vulnerability', 'arbitrary code', 'internet explorer']\n",
      "['Topic_1', 'hkcu software', 'regwrite hkcu', 'software microsoft', 'microsoft office', '111 110', 'office powerpoint', '100 100', 'wscript shell', 'createobject wscript', 'code snippet', 'root cimv2', 'getobject winmgmts', '111 114', 'pastebin raw', 'shell regwrite', 'microsoft windows', '110 103', 'create powershell', 'figure piece', 'winmgmts impersonationlevel']\n",
      "['Topic_2', 'cyber security', 'executive search', 'risk management', 'security risk', 'search firm', 'incident response', 'network security', 'penetration testing', 'security security', 'access management', 'cloud security', 'application security', 'identity access', 'security management', 'firm specializing', 'specialties security', 'management security', 'specialties ciso', 'managing partner', 'fortune 500']\n",
      "['Topic_3', 'remote work', 'coronavirus pandemic', 'security teams', 'security pros', 'amid covid', '2020 covid', 'pandemic security', 'pandemic 2020', 'remote access', 'ransomware attacks', 'remote workforce', 'dark reading', 'work security', 'security practitioners', 'business continuity', 'post pandemic', 'covid themed', 'phishing campaigns', 'covid crisis', 'malware campaign']\n",
      "['Topic_4', 'kevin roose', 'archived recording', 'michael barbaro', 'lives matter', 'black lives', 'mark zuckerberg', 'president trump', 'social media', 'candace owens', 'barbaro kevin', 'recording mark', 'recording candace', 'roose twitter', 'roose facebook', 'roose yeah', 'george floyd', 'fact checking', 'facebook twitter', 'political ads', 'jack dorsey']\n",
      "['Topic_5', 'cve 2020', 'vulnerability microsoft', 'microsoft windows', 'elevation privilege', 'privilege vulnerability', 'windows cve', 'code execution', 'remote code', 'execution vulnerability', 'service elevation', 'microsoft office', 'office sharepoint', 'sharepoint cve', 'graphics component', 'windows state', 'state repository', 'repository service', 'windows runtime', 'runtime elevation', 'microsoft graphics']\n",
      "['Topic_6', '2020 virtual', 'cybersecurity conference', 'ciso executive', 'executive summit', 'virtual august', 'conference virtual', 'virtual cybersecurity', 'virtual june', 'august sans', 'june cancelled', 'june 2020', 'virtual july', '2020 cybersecurity', 'summit q2', 'summer 2020', 'management summit', 'cybersecurity collaboration', 'collaboration forum', 'cybersecurity summit', 'risk management']\n",
      "['Topic_7', 'contact tracing', 'tracing apps', 'public health', 'apple google', 'tracing app', 'google apple', 'wade roush', 'exposure notification', 'covid contact', 'tracing efforts', 'privacy security', 'personal data', 'health officials', 'technology review', 'open source', 'privacy concerns', 'manual tracing', 'health authorities', 'san francisco', 'contact tracers']\n",
      "['Topic_8', 'cve cve', 'cve 2017', 'cve 2019', 'cve 2016', 'open port', 'cve 2018', 'tcp open', 'discovered open', 'microsoft windows', 'cve 2020', 'cve 2015', 'https nmap', 'nmap org', 'vulnerability scanner', 'nse script', 'cve 2014', 'windows server', 'nmap scan', 'nmap https', '443 tcp']\n",
      "['Topic_9', 'jim salter', 'salter jim', 'developer edition', 'threat modeling', 'xps developer', 'battery life', 'ice lake', 'ubuntu 04', 'dragonfly elite', 'killer ax1650', 'dell xps', 'google chrome', '4k uhd', 'desktop environment', '04 lts', 'thunderbolt port', '2020 model', 'xps 2020', 'ax1650 card', 'modern office']\n",
      "\n",
      "Top 10 most numerous terms:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47579</td>\n",
       "      <td>cve 2020</td>\n",
       "      <td>1373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41456</td>\n",
       "      <td>contact tracing</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163607</td>\n",
       "      <td>social media</td>\n",
       "      <td>620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186071</td>\n",
       "      <td>united states</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>178819</td>\n",
       "      <td>threat actors</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>43422</td>\n",
       "      <td>coronavirus pandemic</td>\n",
       "      <td>508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44795</td>\n",
       "      <td>covid pandemic</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34560</td>\n",
       "      <td>code execution</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48532</td>\n",
       "      <td>dark reading</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>101063</td>\n",
       "      <td>law enforcement</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                  term  freq\n",
       "0   47579              cve 2020  1373\n",
       "1   41456       contact tracing   740\n",
       "2  163607          social media   620\n",
       "3  186071         united states   611\n",
       "4  178819         threat actors   515\n",
       "5   43422  coronavirus pandemic   508\n",
       "6   44795        covid pandemic   507\n",
       "7   34560        code execution   499\n",
       "8   48532          dark reading   438\n",
       "9  101063       law enforcement   426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use count based vectorizer\n",
    "if vectorizer_to_use ==1:\n",
    "    vectorizer = CountVectorizer(stop_words = custom_stop_words, min_df = 2, analyzer='word', ngram_range=(ngram, ngram))\n",
    "else:\n",
    "    # or use TF-IDF based vectorizer\n",
    "    vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words=custom_stop_words, analyzer='word', ngram_range=(ngram, ngram))\n",
    "\n",
    "doc_term_matrix = vectorizer.fit_transform(raw_documents)\n",
    "print( \"Created %d X %d document-term matrix in variable doc_term_matrix\\n\" % (doc_term_matrix.shape[0], doc_term_matrix.shape[1]) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "vocab = vectorizer.vocabulary_ #This is the dict from which you can pull the words, eg vocab['ocean']\n",
    "terms = vectorizer.get_feature_names() #Just the list equivalent of vocab, indexed in the same order\n",
    "print(\"Vocabulary has %d distinct terms, examples below \" % len(terms))\n",
    "print(terms[500:550], '\\n')\n",
    "\n",
    "term_frequency_table = pd.DataFrame({'term': terms,'freq': list(np.array(doc_term_matrix.sum(axis=0)).reshape(-1))})\n",
    "term_frequency_table = term_frequency_table.sort_values(by='freq', ascending=False).reset_index()\n",
    "\n",
    "freq_df = pd.DataFrame(doc_term_matrix.todense(), columns = terms)\n",
    "freq_df = freq_df.sum(axis=0)\n",
    "freq_df = freq_df.sort_values(ascending=False)\n",
    "\n",
    "# create the model\n",
    "# Pick between NMF or LDA methods (don't know what they are, try whichever gives better results)\n",
    "if NMF_or_LDA == 'nmf':\n",
    "    model = NMF( init=\"nndsvd\", n_components=num_topics ) \n",
    "else:\n",
    "    model = LatentDirichletAllocation(n_components=num_topics, learning_method='online') \n",
    "    \n",
    "# apply the model and extract the two factor matrices\n",
    "W = model.fit_transform( doc_term_matrix ) #This matrix has docs as rows and k-topics as columns\n",
    "H = model.components_ #This matrix has k-topics as rows and vocab as columns\n",
    "print('Shape of W is', W.shape, 'docs as rows and', num_topics, 'topics as columns. First row below')\n",
    "print(W[0].round(1))\n",
    "print('\\nShape of H is', H.shape, num_topics, 'topics as rows and vocab as columns. First row below')\n",
    "print(H[0].round(1))\n",
    "\n",
    "# Check which document belongs to which topic, and print value_count\n",
    "topic_for_doc_df = pd.DataFrame(columns = ['article', 'topic', 'value'])\n",
    "for i in range(W.shape[0]):\n",
    "    a = W[i] \n",
    "    b = np.argsort(a)[::-1]\n",
    "    temp_df = pd.DataFrame({'article': [i], 'topic':['Topic_'+str(b[0])], 'value': [a[b[0]]]})\n",
    "    topic_for_doc_df = pd.concat([topic_for_doc_df, temp_df])\n",
    "\n",
    "top_docs_for_topic_df = pd.DataFrame(columns = ['topic', 'doc_number', 'weight'])    \n",
    "for i in range(W.shape[1]):\n",
    "    topic = i\n",
    "    temp_df = pd.DataFrame({'topic': ['Topic_'+str(i) for x in range(W.shape[0])], \n",
    "                            'doc_number':  list(range(W.shape[0])), \n",
    "                            'weight': list(W[:,i])})\n",
    "    temp_df = temp_df.sort_values(by=['topic', 'weight'], ascending=[True, False])\n",
    "    top_docs_for_topic_df = pd.concat([top_docs_for_topic_df, temp_df])\n",
    "# Add text to the top_docs dataframe as a new column\n",
    "top_docs_for_topic_df['text']=[raw_documents[i] for i in list(top_docs_for_topic_df.doc_number)] \n",
    "# Print top two docs for each topic\n",
    "print('\\nTop documents for each topic')\n",
    "print(top_docs_for_topic_df.groupby('topic').head(2))\n",
    "\n",
    "print('\\n')\n",
    "print('Topic number and counts of documents against each:')\n",
    "print(topic_for_doc_df.topic.value_counts())\n",
    "\n",
    "# Create dataframe with top-10 words for each topic\n",
    "words_in_topics_df = pd.DataFrame(columns = ['topic', 'words', 'freq'])\n",
    "for i in range(H.shape[0]):\n",
    "    a = H[i] \n",
    "    b = np.argsort(a)[::-1]\n",
    "    np.array(b[:top_n_words])\n",
    "    words = [terms[i] for i in b[:top_n_words]]\n",
    "    freq = [a[i] for i in b[:top_n_words]]\n",
    "    temp_df = pd.DataFrame({'topic':'Topic_'+str(i), 'words': words, 'freq': freq})\n",
    "    words_in_topics_df = pd.concat([words_in_topics_df, temp_df])\n",
    "\n",
    "print('\\n')\n",
    "print('Top', top_n_words, 'words dataframe with weights')\n",
    "print(words_in_topics_df.head(10))\n",
    "\n",
    "\n",
    "\n",
    "# print as list\n",
    "print('\\nSame list as above as a list')\n",
    "words_in_topics_list = words_in_topics_df.groupby('topic')['words'].apply(list)\n",
    "lala =[]\n",
    "for i in range(len(words_in_topics_list)):\n",
    "    a = [list(words_in_topics_list.index)[i]]\n",
    "    b = words_in_topics_list[i]\n",
    "    lala = lala + [a+b]\n",
    "    print(a + b) \n",
    "    \n",
    "    \n",
    "# Top terms\n",
    "print('\\nTop 10 most numerous terms:')\n",
    "term_frequency_table.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cve 2020                 1373\n",
       "contact tracing           740\n",
       "social media              620\n",
       "united states             611\n",
       "threat actors             515\n",
       "                         ... \n",
       "national cable              2\n",
       "national broadcasting       2\n",
       "national banks              2\n",
       "national approach           2\n",
       "lamphone practical          2\n",
       "Length: 199469, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>term</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30860</th>\n",
       "      <td>24569</td>\n",
       "      <td>security</td>\n",
       "      <td>11899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30859</th>\n",
       "      <td>8062</td>\n",
       "      <td>data</td>\n",
       "      <td>8251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30858</th>\n",
       "      <td>6621</td>\n",
       "      <td>company</td>\n",
       "      <td>6059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30857</th>\n",
       "      <td>683</td>\n",
       "      <td>2020</td>\n",
       "      <td>5440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30856</th>\n",
       "      <td>27848</td>\n",
       "      <td>time</td>\n",
       "      <td>4716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3822</th>\n",
       "      <td>25086</td>\n",
       "      <td>shrunk</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3823</th>\n",
       "      <td>25091</td>\n",
       "      <td>shuffled</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3824</th>\n",
       "      <td>25001</td>\n",
       "      <td>shook</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3825</th>\n",
       "      <td>29737</td>\n",
       "      <td>vodavi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30860</td>\n",
       "      <td>寒舟</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30861 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index      term   freq\n",
       "30860  24569  security  11899\n",
       "30859   8062      data   8251\n",
       "30858   6621   company   6059\n",
       "30857    683      2020   5440\n",
       "30856  27848      time   4716\n",
       "...      ...       ...    ...\n",
       "3822   25086    shrunk      2\n",
       "3823   25091  shuffled      2\n",
       "3824   25001     shook      2\n",
       "3825   29737    vodavi      2\n",
       "0      30860        寒舟      2\n",
       "\n",
       "[30861 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency_table.sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary_x</th>\n",
       "      <th>URL</th>\n",
       "      <th>published</th>\n",
       "      <th>keywords</th>\n",
       "      <th>summary_y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friday Squid Blogging: Fishing for Jumbo Squid</td>\n",
       "      <td>Interesting article on the rise of the jumbo s...</td>\n",
       "      <td>https://www.schneier.com/blog/archives/2020/06...</td>\n",
       "      <td>2020-06-26 20:57:09+00:00</td>\n",
       "      <td>post,squidinteresting,talk,posting,rise,usual,...</td>\n",
       "      <td>Friday Squid Blogging: Fishing for Jumbo Squid...</td>\n",
       "      <td>Friday Squid Blogging: Fishing for Jumbo Squid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Unintended Harms of Cybersecurity</td>\n",
       "      <td>Interesting research: \"Identifying Unintended ...</td>\n",
       "      <td>https://www.schneier.com/blog/archives/2020/06...</td>\n",
       "      <td>2020-06-26 12:00:59+00:00</td>\n",
       "      <td>harms,countermeasures,unintended,consequences,...</td>\n",
       "      <td>Interesting research: \"Identifying Unintended ...</td>\n",
       "      <td>Interesting research: \"Identifying Unintended ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analyzing IoT Security Best Practices</td>\n",
       "      <td>New research: \"Best Practices for IoT Security...</td>\n",
       "      <td>https://www.schneier.com/blog/archives/2020/06...</td>\n",
       "      <td>2020-06-25 12:09:36+00:00</td>\n",
       "      <td>practices,follow,iot,recommendations,specific,...</td>\n",
       "      <td>Analyzing IoT Security Best PracticesNew resea...</td>\n",
       "      <td>Analyzing IoT Security Best Practices\\n\\nNew r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COVID-19 Risks of Flying</td>\n",
       "      <td>I fly a lot. Over the past five years, my aver...</td>\n",
       "      <td>https://www.schneier.com/blog/archives/2020/06...</td>\n",
       "      <td>2020-06-24 17:32:30+00:00</td>\n",
       "      <td>schneier,wearing,mask,wear,think,person,lot,mo...</td>\n",
       "      <td>This is all a prelude to saying that I have be...</td>\n",
       "      <td>COVID-19 Risks of Flying\\n\\nI fly a lot. Over ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cryptocurrency Pump and Dump Scams</td>\n",
       "      <td>Really interesting research: \"An examination o...</td>\n",
       "      <td>https://www.schneier.com/blog/archives/2020/06...</td>\n",
       "      <td>2020-06-24 11:30:32+00:00</td>\n",
       "      <td>pump,schemes,schneier,scope,cryptocurrencies,u...</td>\n",
       "      <td>Really interesting research: \"An examination o...</td>\n",
       "      <td>Really interesting research: \"An examination o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Amazon confirms a major outbreak of COVID-19 a...</td>\n",
       "      <td>&lt;A HREF=\"https://www.businessinsider.com/amazo...</td>\n",
       "      <td>http://www.techmeme.com/200423/p38#a200423p38</td>\n",
       "      <td>2020-04-23 23:55:23+00:00</td>\n",
       "      <td>video,past,security,warehouse,outbreak,surge,z...</td>\n",
       "      <td>— Zoom Video Communications Inc. has been lamb...</td>\n",
       "      <td>— Zoom Video Communications Inc. has been lamb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>The coronavirus is accelerating the shift to a...</td>\n",
       "      <td>&lt;A HREF=\"https://www.ft.com/content/990e89de-8...</td>\n",
       "      <td>http://www.techmeme.com/200423/p33#a200423p33</td>\n",
       "      <td>2020-04-23 21:30:02+00:00</td>\n",
       "      <td>shift,financial,right,whiteboards,coronavirus,...</td>\n",
       "      <td>— Miro is a company in the right place at the ...</td>\n",
       "      <td>— Miro is a company in the right place at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Doctors are using AI to triage covid-19 patien...</td>\n",
       "      <td>Rizwan Malik had always had an interest in AI....</td>\n",
       "      <td>https://www.technologyreview.com/2020/04/23/10...</td>\n",
       "      <td>2020-04-23 14:00:00+00:00</td>\n",
       "      <td>pandemic,health,system,triage,using,coronaviru...</td>\n",
       "      <td>The pandemic, in other words, has turned into ...</td>\n",
       "      <td>The Royal Bolton Hospital is among a growing n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>The race to save the first draft of coronaviru...</td>\n",
       "      <td>Eight years ago, Suleika Jaouad was alone in a...</td>\n",
       "      <td>https://www.technologyreview.com/2020/04/21/99...</td>\n",
       "      <td>2020-04-21 09:00:00+00:00</td>\n",
       "      <td>save,pandemic,life,internet,history,coronaviru...</td>\n",
       "      <td>But getting the internet to archive as much as...</td>\n",
       "      <td>Within a week, Blair’s tweet got the attention...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>The US needs to do 20 million tests a day to r...</td>\n",
       "      <td>The news: A group of experts has produced a pl...</td>\n",
       "      <td>https://www.technologyreview.com/2020/04/20/10...</td>\n",
       "      <td>2020-04-20 10:40:47+00:00</td>\n",
       "      <td>safely,way,day,20,plan,million,test,reopen,tes...</td>\n",
       "      <td>The news: A group of experts has produced a pl...</td>\n",
       "      <td>The news: A group of experts has produced a pl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5125 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0       Friday Squid Blogging: Fishing for Jumbo Squid   \n",
       "1                The Unintended Harms of Cybersecurity   \n",
       "2                Analyzing IoT Security Best Practices   \n",
       "3                             COVID-19 Risks of Flying   \n",
       "4                   Cryptocurrency Pump and Dump Scams   \n",
       "..                                                 ...   \n",
       "341  Amazon confirms a major outbreak of COVID-19 a...   \n",
       "346  The coronavirus is accelerating the shift to a...   \n",
       "356  Doctors are using AI to triage covid-19 patien...   \n",
       "364  The race to save the first draft of coronaviru...   \n",
       "365  The US needs to do 20 million tests a day to r...   \n",
       "\n",
       "                                             summary_x  \\\n",
       "0    Interesting article on the rise of the jumbo s...   \n",
       "1    Interesting research: \"Identifying Unintended ...   \n",
       "2    New research: \"Best Practices for IoT Security...   \n",
       "3    I fly a lot. Over the past five years, my aver...   \n",
       "4    Really interesting research: \"An examination o...   \n",
       "..                                                 ...   \n",
       "341  <A HREF=\"https://www.businessinsider.com/amazo...   \n",
       "346  <A HREF=\"https://www.ft.com/content/990e89de-8...   \n",
       "356  Rizwan Malik had always had an interest in AI....   \n",
       "364  Eight years ago, Suleika Jaouad was alone in a...   \n",
       "365  The news: A group of experts has produced a pl...   \n",
       "\n",
       "                                                   URL  \\\n",
       "0    https://www.schneier.com/blog/archives/2020/06...   \n",
       "1    https://www.schneier.com/blog/archives/2020/06...   \n",
       "2    https://www.schneier.com/blog/archives/2020/06...   \n",
       "3    https://www.schneier.com/blog/archives/2020/06...   \n",
       "4    https://www.schneier.com/blog/archives/2020/06...   \n",
       "..                                                 ...   \n",
       "341      http://www.techmeme.com/200423/p38#a200423p38   \n",
       "346      http://www.techmeme.com/200423/p33#a200423p33   \n",
       "356  https://www.technologyreview.com/2020/04/23/10...   \n",
       "364  https://www.technologyreview.com/2020/04/21/99...   \n",
       "365  https://www.technologyreview.com/2020/04/20/10...   \n",
       "\n",
       "                    published  \\\n",
       "0   2020-06-26 20:57:09+00:00   \n",
       "1   2020-06-26 12:00:59+00:00   \n",
       "2   2020-06-25 12:09:36+00:00   \n",
       "3   2020-06-24 17:32:30+00:00   \n",
       "4   2020-06-24 11:30:32+00:00   \n",
       "..                        ...   \n",
       "341 2020-04-23 23:55:23+00:00   \n",
       "346 2020-04-23 21:30:02+00:00   \n",
       "356 2020-04-23 14:00:00+00:00   \n",
       "364 2020-04-21 09:00:00+00:00   \n",
       "365 2020-04-20 10:40:47+00:00   \n",
       "\n",
       "                                              keywords  \\\n",
       "0    post,squidinteresting,talk,posting,rise,usual,...   \n",
       "1    harms,countermeasures,unintended,consequences,...   \n",
       "2    practices,follow,iot,recommendations,specific,...   \n",
       "3    schneier,wearing,mask,wear,think,person,lot,mo...   \n",
       "4    pump,schemes,schneier,scope,cryptocurrencies,u...   \n",
       "..                                                 ...   \n",
       "341  video,past,security,warehouse,outbreak,surge,z...   \n",
       "346  shift,financial,right,whiteboards,coronavirus,...   \n",
       "356  pandemic,health,system,triage,using,coronaviru...   \n",
       "364  save,pandemic,life,internet,history,coronaviru...   \n",
       "365  safely,way,day,20,plan,million,test,reopen,tes...   \n",
       "\n",
       "                                             summary_y  \\\n",
       "0    Friday Squid Blogging: Fishing for Jumbo Squid...   \n",
       "1    Interesting research: \"Identifying Unintended ...   \n",
       "2    Analyzing IoT Security Best PracticesNew resea...   \n",
       "3    This is all a prelude to saying that I have be...   \n",
       "4    Really interesting research: \"An examination o...   \n",
       "..                                                 ...   \n",
       "341  — Zoom Video Communications Inc. has been lamb...   \n",
       "346  — Miro is a company in the right place at the ...   \n",
       "356  The pandemic, in other words, has turned into ...   \n",
       "364  But getting the internet to archive as much as...   \n",
       "365  The news: A group of experts has produced a pl...   \n",
       "\n",
       "                                                  text  \n",
       "0    Friday Squid Blogging: Fishing for Jumbo Squid...  \n",
       "1    Interesting research: \"Identifying Unintended ...  \n",
       "2    Analyzing IoT Security Best Practices\\n\\nNew r...  \n",
       "3    COVID-19 Risks of Flying\\n\\nI fly a lot. Over ...  \n",
       "4    Really interesting research: \"An examination o...  \n",
       "..                                                 ...  \n",
       "341  — Zoom Video Communications Inc. has been lamb...  \n",
       "346  — Miro is a company in the right place at the ...  \n",
       "356  The Royal Bolton Hospital is among a growing n...  \n",
       "364  Within a week, Blair’s tweet got the attention...  \n",
       "365  The news: A group of experts has produced a pl...  \n",
       "\n",
       "[5125 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ron Jeremy',\n",
       " 'Celta Vigo vs Barcelona',\n",
       " 'Andrew Toles',\n",
       " 'Mike Henry',\n",
       " 'Alia Shawkat',\n",
       " 'Margot Robbie',\n",
       " 'FA Cup',\n",
       " 'Milton Glaser',\n",
       " 'Pierce Brosnan',\n",
       " 'Mike Pence',\n",
       " 'Unilever',\n",
       " 'Phil Mickelson',\n",
       " 'Dr Disrespect',\n",
       " 'Eurovision movie',\n",
       " 'Huey. Rapper',\n",
       " 'Timothee Chalamet',\n",
       " 'Siya Kakkar',\n",
       " 'Imperial County',\n",
       " 'Washington, D.C',\n",
       " 'Gap stock']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaper.hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
