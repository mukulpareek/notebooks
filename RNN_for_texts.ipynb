{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing with an RNN  \n",
    "  \n",
    "In order to get the RNN to consume text data, we need two things:  \n",
    "  \n",
    "1. A list of all the texts, eg `texts = ['Today is a nice day', 'Yesterday was gorgeous',....]`  \n",
    "2. We need the labels in a list too, eg `labels = [1, 0, 1, 1,...]`.  If labels are text string categories, then use one-hot encoding to convert them into an array, with each row of the array corresponding to an observation.  \n",
    "  \n",
    "We first set up a tokenizer as follows:  \n",
    "  \n",
    "```python\n",
    "from keras.preprocessing.text import Tokenizer  \n",
    "from keras.preprocessing.sequence import pad_sequences  \n",
    "  \n",
    "tokenizer = Tokenizer(num_words = vocab_size) #Sets up a blank tokenizer  \n",
    "tokenizer.fit_on_texts(texts)   \n",
    "# fit_on_texts creates the vocabulary based on `texts`, based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot).  \n",
    "  \n",
    "sequences = tokenizer.texts_to_sequences(texts)  \n",
    "# This step transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.   \n",
    "  \n",
    "word_index = tokenizer.word_index # This is the dictionary of words and their indexes which one can see using  word_index.items()  \n",
    "  \n",
    "data = pad_sequences(sequences, maxlen = maxlen) # Here we make all texts equal in length, and padding is pre (can be changed   by padding='post')  \n",
    "  \n",
    "labels = np.asarray(labels) # We convert the labels into an array from its original list form.  \n",
    "\n",
    "```  \n",
    "  \n",
    "At this point all our input text data exists as perfectly sized tensors in the variable called `data`, and `labels`.  The `data` variable has references to the `word_index` which is essentially the dictionary for the task.  \n",
    "  \n",
    "Next, we need to create the embeddings_matrix based on Glove.  For this, we first load up Glove in a giant matrix, and then we look up each item in the `word_index` in the Glove matrix and create our pre-trained embeddings_matrix based on Glove.  In this matrix, we preserve the index order as in the `word_index`, so what is #3 in the word index is also the 3rd row in the embeddings_matrix.  The 0th row is all zeros, and only the first row onwards are used because in the `word_index` dictionary there is no entry for zero.  \n",
    "  \n",
    "Now we are good to go to feed this into our NN. The pretrained embeddings_matrix makes its way into the NN using the following commands:  \n",
    "  \n",
    "```python\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False\n",
    "```\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "imdb_dir = '\\\\Users\\\\user\\\\aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels=[]\n",
    "texts=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the text files\n",
    "for label_type in ['neg','pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:]== '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            #print(fname)\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type =='neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the variable `texts` contains all the reviews in plaintext as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88588 unique tokens\n",
      "Shape of data tensor (25000, 500)\n",
      "Shape of label tensor (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen=500\n",
    "training_samples=20000\n",
    "validation_samples = 10000\n",
    "vocab_size=10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen = maxlen)\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor', data.shape)\n",
    "print('Shape of label tensor', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples:training_samples+validation_samples]\n",
    "y_val = labels[training_samples:training_samples+validation_samples]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 words and corresponding vectors\n"
     ]
    }
   ],
   "source": [
    "glove_dir = '\\\\Users\\\\user\\\\glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "#f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "f=open('\\\\Users\\\\user\\\\glove.6B\\\\glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s words and corresponding vectors' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "       -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "        0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "       -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "        0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "       -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "        0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "        0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "       -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "       -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "       -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "       -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "       -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "       -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "       -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "        0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "       -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_index.get('th13e'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding matrix based on Glove\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < vocab_size:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the `embedding_matrix` has one row per word in the vocabulary.  Each row has the vector for that word, picked from glove.  Because it is an np.array, it has no row or column names. The order of the words in the rows is the same as the order of words in the dict word_index.  \n",
    "  \n",
    "We will feed this embedding matrix as weights to the embedding layer.  \n",
    "  \n",
    "## Build the model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM, SimpleRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,017,057\n",
      "Trainable params: 1,017,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim)) # Note that vocab_size=10000 (vocab size), embedding_dim = 100 (100 dense vector for each word from Glove), maxlen=100 (using only first 100 words of each review)\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/15\n",
      "16000/16000 [==============================] - 159s 10ms/step - loss: 0.3427 - acc: 0.8536 - val_loss: 0.3584 - val_acc: 0.8438\n",
      "Epoch 2/15\n",
      "16000/16000 [==============================] - 168s 10ms/step - loss: 0.3253 - acc: 0.8601 - val_loss: 0.3589 - val_acc: 0.8393\n",
      "Epoch 3/15\n",
      "16000/16000 [==============================] - 168s 10ms/step - loss: 0.3148 - acc: 0.8673 - val_loss: 0.3514 - val_acc: 0.8450\n",
      "Epoch 4/15\n",
      "16000/16000 [==============================] - 172s 11ms/step - loss: 0.3071 - acc: 0.8708 - val_loss: 0.3423 - val_acc: 0.8540\n",
      "Epoch 5/15\n",
      "16000/16000 [==============================] - 181s 11ms/step - loss: 0.2994 - acc: 0.8742 - val_loss: 0.3485 - val_acc: 0.8540\n",
      "Epoch 6/15\n",
      "16000/16000 [==============================] - 170s 11ms/step - loss: 0.2894 - acc: 0.8796 - val_loss: 0.3447 - val_acc: 0.8593\n",
      "Epoch 7/15\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.2814 - acc: 0.8829 - val_loss: 0.3421 - val_acc: 0.8577\n",
      "Epoch 8/15\n",
      "16000/16000 [==============================] - 166s 10ms/step - loss: 0.2739 - acc: 0.8869 - val_loss: 0.3490 - val_acc: 0.8518\n",
      "Epoch 9/15\n",
      "16000/16000 [==============================] - 166s 10ms/step - loss: 0.2659 - acc: 0.8911 - val_loss: 0.3353 - val_acc: 0.8652\n",
      "Epoch 10/15\n",
      "16000/16000 [==============================] - 164s 10ms/step - loss: 0.2572 - acc: 0.8950 - val_loss: 0.3404 - val_acc: 0.8633\n",
      "Epoch 11/15\n",
      "16000/16000 [==============================] - 166s 10ms/step - loss: 0.2509 - acc: 0.8992 - val_loss: 0.3490 - val_acc: 0.8497\n",
      "Epoch 12/15\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.2479 - acc: 0.9011 - val_loss: 0.3362 - val_acc: 0.8605\n",
      "Epoch 13/15\n",
      "16000/16000 [==============================] - 167s 10ms/step - loss: 0.2398 - acc: 0.9048 - val_loss: 0.3277 - val_acc: 0.8608\n",
      "Epoch 14/15\n",
      "16000/16000 [==============================] - 166s 10ms/step - loss: 0.2339 - acc: 0.9060 - val_loss: 0.3297 - val_acc: 0.8640\n",
      "Epoch 15/15\n",
      "16000/16000 [==============================] - 165s 10ms/step - loss: 0.2260 - acc: 0.9100 - val_loss: 0.3428 - val_acc: 0.8630\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
