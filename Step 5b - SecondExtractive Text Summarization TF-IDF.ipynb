{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://medium.com/voice-tech-podcast/automatic-extractive-text-summarization-using-tfidf-3fc9a7b26f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Percentage of information to retain(in percent): 25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "\n",
      "\n",
      "Summary:\n",
      "Friday Squid Blogging: Fishing for Jumbo Squid\n",
      "\n",
      "Interesting article on the rise of the jumbo squid industry as a result of climate change. As usual, you can also use this squid post to talk about the security stories in the news that I haven't covered. Read my blog posting guidelines here. We hope that our work provides a basis for the community to build on in order to better understand best practices, identify and reach consensus on specific practices, and then find ways to motivate relevant stakeholders to follow them. The \"less than six feet, more than ten minutes\" model has given way to a much more sophisticated model involving airflow, the level of virus in the room, and the viral load in the person who might be infected. Airlines are trying to make things better: blocking middle seats, serving less food and drink, trying to get people to wear masks. (This video is worth watching.) I've started to see airlines requiring masks and banning those who won't, and not just strongly encouraging them. (If mask wearing is treated the same as the seat belt wearing, it will make a huge difference.) (It doesn't help that I get a -1 on my COVID saving throw for type A blood, and another -1 for male pattern baldness. On the other hand, I think I get a +3 Constitution bonus. And everyone: wear a mask, and wash your hands.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import operator\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "wordlemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(words):\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
    "    return lemmatized_words\n",
    "def stem_words(words):\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "       stemmed_words.append(stemmer.stem(word))\n",
    "    return stemmed_words\n",
    "def remove_special_characters(text):\n",
    "    regex = r'[^a-zA-Z0-9\\s]'\n",
    "    text = re.sub(regex,'',text)\n",
    "    return text\n",
    "def freq(words):\n",
    "    words = [word.lower() for word in words]\n",
    "    dict_freq = {}\n",
    "    words_unique = []\n",
    "    for word in words:\n",
    "       if word not in words_unique:\n",
    "           words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "       dict_freq[word] = words.count(word)\n",
    "    return dict_freq\n",
    "def pos_tagging(text):\n",
    "    pos_tag = nltk.pos_tag(text.split())\n",
    "    pos_tagged_noun_verb = []\n",
    "    for word,tag in pos_tag:\n",
    "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
    "             pos_tagged_noun_verb.append(word)\n",
    "    return pos_tagged_noun_verb\n",
    "def tf_score(word,sentence):\n",
    "    freq_sum = 0\n",
    "    word_frequency_in_sentence = 0\n",
    "    len_sentence = len(sentence)\n",
    "    for word_in_sentence in sentence.split():\n",
    "        if word == word_in_sentence:\n",
    "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
    "    tf =  word_frequency_in_sentence/ len_sentence\n",
    "    return tf\n",
    "def idf_score(no_of_sentences,word,sentences):\n",
    "    no_of_sentence_containing_word = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = remove_special_characters(str(sentence))\n",
    "        sentence = re.sub(r'\\d+', '', sentence)\n",
    "        sentence = sentence.split()\n",
    "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
    "        if word in sentence:\n",
    "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
    "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
    "    return idf\n",
    "def tf_idf_score(tf,idf):\n",
    "    return tf*idf\n",
    "def word_tfidf(dict_freq,word,sentences,sentence):\n",
    "    word_tfidf = []\n",
    "    tf = tf_score(word,sentence)\n",
    "    idf = idf_score(len(sentences),word,sentences)\n",
    "    tf_idf = tf_idf_score(tf,idf)\n",
    "    return tf_idf\n",
    "def sentence_importance(sentence,dict_freq,sentences):\n",
    "     sentence_score = 0\n",
    "     sentence = remove_special_characters(str(sentence)) \n",
    "     sentence = re.sub(r'\\d+', '', sentence)\n",
    "     pos_tagged_sentence = [] \n",
    "     no_of_sentences = len(sentences)\n",
    "     pos_tagged_sentence = pos_tagging(sentence)\n",
    "     for word in pos_tagged_sentence:\n",
    "          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n",
    "                word = word.lower()\n",
    "                word = wordlemmatizer.lemmatize(word)\n",
    "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
    "     return sentence_score\n",
    "file = 'input.txt'\n",
    "file = open(file , 'r')\n",
    "text = file.read()\n",
    "tokenized_sentence = sent_tokenize(text)\n",
    "text = remove_special_characters(str(text))\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "tokenized_words_with_stopwords = word_tokenize(text)\n",
    "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
    "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
    "tokenized_words = [word.lower() for word in tokenized_words]\n",
    "tokenized_words = lemmatize_words(tokenized_words)\n",
    "word_freq = freq(tokenized_words)\n",
    "input_user = int(input('Percentage of information to retain(in percent):'))\n",
    "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
    "print(no_of_sentences)\n",
    "c = 1\n",
    "sentence_with_importance = {}\n",
    "for sent in tokenized_sentence:\n",
    "    sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n",
    "    sentence_with_importance[c] = sentenceimp\n",
    "    c = c+1\n",
    "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
    "cnt = 0\n",
    "summary = []\n",
    "sentence_no = []\n",
    "for word_prob in sentence_with_importance:\n",
    "    if cnt < no_of_sentences:\n",
    "        sentence_no.append(word_prob[0])\n",
    "        cnt = cnt+1\n",
    "    else:\n",
    "      break\n",
    "sentence_no.sort()\n",
    "cnt = 1\n",
    "for sentence in tokenized_sentence:\n",
    "    if cnt in sentence_no:\n",
    "       summary.append(sentence)\n",
    "    cnt = cnt+1\n",
    "summary = \" \".join(summary)\n",
    "print(\"\\n\")\n",
    "print(\"Summary:\")\n",
    "print(summary)\n",
    "outF = open('summary.txt',\"w\")\n",
    "outF.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
